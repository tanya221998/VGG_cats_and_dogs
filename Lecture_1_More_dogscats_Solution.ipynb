{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **In this short assignment, you will be asked to fine-tune the pretrained VGG model to classify 37 categories of cat and dog breeds.**\n",
        "\n",
        "\n",
        "\n",
        "Please submit your code and answers to the questions in the form of a Jupyter notebook, containing Pytorch code with explanations, along with a Markdown text explaining different parts if needed."
      ],
      "metadata": {
        "id": "0gQUymSZhxdh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SvfKxAGCdhF"
      },
      "source": [
        "# Short Assignment 1: More dogs and cats!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI1VQlwECdhG"
      },
      "source": [
        "This time, you are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds. You will need to adapt the code from lesson 1 to this new task, i.e. a classification with 37 categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4dAhx8-CdhM"
      },
      "source": [
        "##  Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oU4Z_DPCdhM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models,transforms,datasets\n",
        "import time\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A66_r51xCdhS"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN3FFTFhHQyi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuej9DPjCdhX"
      },
      "source": [
        "Check if GPU is available and if not change the [runtime](https://www.geeksforgeeks.org/how-to-use-google-colab/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t56d0zbFCdhY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Using gpu: %s ' % torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDLlOjT5Et4p"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "The data given on the website [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) is made of two files: `images.tar.gz` and `annotations.tar.gz`. We first need to download and decompress these files.\n",
        "\n",
        "Depending if you use google colab or your own computer, you can adapt the code below to choose where to store the data.\n",
        "\n",
        "To see where you are, you can use the standard unix comands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEAr5HNeZCjm"
      },
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnn5pLK6EyJK"
      },
      "outputs": [],
      "source": [
        "%mkdir data\n",
        "# the line below needs to be adapted if not running on google colab\n",
        "%cd ./data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkQKDwnfZCjv"
      },
      "source": [
        "Now that you are in the right directory, you can download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTobJ9vTE37J"
      },
      "outputs": [],
      "source": [
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEHXmG2vZCjy"
      },
      "source": [
        "and uncompress it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsMtmnCbCdhd"
      },
      "outputs": [],
      "source": [
        "!tar zxvf images.tar.gz\n",
        "!tar zxvf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXlQ2ojZCj0"
      },
      "source": [
        "Check that everything went correctly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUlawYthZCj1"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLio3SAbZCj3"
      },
      "source": [
        "## Warning\n",
        "\n",
        "If you are running this notebook on your own computer, you need to download the data only once. If you want to run this notebook a second time, you can safely skip this section and the section below as your dataset will be stored nicely on your computer.\n",
        "\n",
        "If you are running this notebook on google colab, you need to download the data and to do the data wrangling each time you are running this notebook as data will be cleared once you log off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPiTo762ZCj3"
      },
      "source": [
        "## 1. Data wrangling\n",
        "\n",
        "You will first need to do a bit of [data wrangling](https://en.wikipedia.org/wiki/Data_wrangling) to organize your dataset in order to use the PyTorch `dataloader`.\n",
        "\n",
        "If you want to understand how the files are organized, have a look at the `README` file in the folder `annotations`.\n",
        "\n",
        "First, we need to split the dataset in a test set and train/validation set. For this, we can use the files `annotations/test.txt` and `annotations/trainval.txt` containing the names of images contained in the test and train/validation sets of the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqNGCio0ZCj3"
      },
      "outputs": [],
      "source": [
        "!head annotations/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tejOC_TZZCj5"
      },
      "outputs": [],
      "source": [
        "!head annotations/trainval.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O0BRsbmZCj7"
      },
      "source": [
        "Above you see that the authors of the original paper made a partition of the dataset: `./images/Abyssinian_201.jpg` is in the test set while `./images/Abyssinian_100.jpg` is in the train/validation set and so on.\n",
        "\n",
        "BTW, if you wonder what Abyssinian means, it is explained [here](https://en.wikipedia.org/wiki/Abyssinian_cat)\n",
        "\n",
        "We first create two directories where we will put images form the test and trainval sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn-tDnZXZCj7"
      },
      "outputs": [],
      "source": [
        "%mkdir test\n",
        "%mkdir trainval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LwIlPrBZCj9"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgorOf_CZCkA"
      },
      "source": [
        "All the images are in the `./images/` folder and you want to store the data according to the following structure:\n",
        "```bash\n",
        ".\n",
        "├── test\n",
        "|   └── Abyssinian # contains images of Abyssinian from the test set\n",
        "|   └── Bengal # contains images of Bengal from the test set\n",
        "|    ...\n",
        "|   └── american_bulldog # contains images of american bulldog from the test set\n",
        "|    ...\n",
        "├── trainval\n",
        "|   └── Abyssinian # contains images of Abyssinian from the trainval set\n",
        "|   └── Bengal # contains images of Bengal from the trainval set\n",
        "|    ...\n",
        "|   └── american_bulldog # contains images of american bulldog from the trainval set\n",
        "|    ...\n",
        "```\n",
        "\n",
        "Note that all images with a name starting with a majuscule is a cat and all images with a name starting with a minuscule is a dog.\n",
        "\n",
        "So here is one way to achieve your task: you will read the `./annotations/test.txt` file line by line; from each line, you will extract the name of the corresponding file and then copy it from the `./images/filename_##.jpg` to `./test/filename/filename_##.jpg`, where `##` is a number.\n",
        "\n",
        "Then you'll do the same thing for `trainval.txt` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAjydL_HZCkA"
      },
      "source": [
        "Below is a little piece of code to show you how to open a file and read it line by line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fiBsTj9ZCkA"
      },
      "outputs": [],
      "source": [
        "# Open the file 'test.txt' in read mode\n",
        "with open('./annotations/test.txt') as fp:\n",
        "    # Read the first line from the file\n",
        "    line = fp.readline()\n",
        "\n",
        "    # Continue looping until there are no more lines to read\n",
        "    while line:\n",
        "        # Split the line into parts using space as the delimiter\n",
        "        # This line splits the current line into parts using a space as the delimiter.\n",
        "        #It expects that each line contains at least four elements.\n",
        "        #The first element is assigned to the variable f,\n",
        "        #while the underscores (_) are used as throwaway variables for the other three elements, meaning they are ignored.\n",
        "        f, _, _, _ = line.split(' ')\n",
        "\n",
        "        # Print the first part of the line\n",
        "        print(f)\n",
        "\n",
        "        # Read the next line from the file\n",
        "        line = fp.readline()  # Read the next line to continue the loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLTktYgZZCkC"
      },
      "source": [
        "In order to remove the `_201` in the example above, you can use the `re` [regular expression lib](https://docs.python.org/3.6/library/re.html) as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ9j71IsZCkD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "pat = re.compile(r'_\\d')\n",
        "res,_ = pat.split(f)\n",
        "print(res)\n",
        "#this is to remove the appending digits at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zVpVOpgZCkE"
      },
      "source": [
        "This small piece of code is useful for creating directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YeBT3CbZCkF"
      },
      "outputs": [],
      "source": [
        "# create directory if it does not exist\n",
        "def check_dir(dir_path):\n",
        "    dir_path = dir_path.replace('//','/')\n",
        "    os.makedirs(dir_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0WzlkPUZCkG"
      },
      "source": [
        "Some more functions that will be useful\n",
        "- for moving files around you can use the `shutil` lib, see [here](https://docs.python.org/3.6/library/shutil.html#shutil.copy)\n",
        "- you can use `os.path.join`\n",
        "- have a look at python [f-string](https://cito.github.io/blog/f-strings/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMHRHKe9ZCkH"
      },
      "outputs": [],
      "source": [
        "import os # import the os module\n",
        "import shutil\n",
        "import re # Assuming 'pat' is a compiled regex, import re\n",
        "\n",
        "# create directory if it does not exist\n",
        "def check_dir(dir_path):\n",
        "    dir_path = dir_path.replace('//','/')\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "#for moving files around"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ciu2GLSZCkJ"
      },
      "outputs": [],
      "source": [
        "# Here you read the ./annotations/test.txt file line by line,\n",
        "# extract the name of the corresponding file\n",
        "# copy the file from the ./images folder\n",
        "# store it in the ./text folder at the right subfolder\n",
        "path_test_dataset = 'test/'\n",
        "with open('./annotations/test.txt') as fp:\n",
        "    line = fp.readline()\n",
        "    while line:\n",
        "        f,_,_,_ = line.split(' ')\n",
        "        res,_ = pat.split(f)\n",
        "        path = os.path.join(path_test_dataset,res)\n",
        "        check_dir(path)  # check and make directory\n",
        "        shutil.copy(f'./images/{f}.jpg',os.path.join(path,f'{f}.jpg')) #here we use python f-string\n",
        "        line = fp.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4jCU3QSZCkK"
      },
      "outputs": [],
      "source": [
        "# Here you do the same thing as above but for trainval data.\n",
        "path_train_dataset='trainval/'\n",
        "with open('./annotations/trainval.txt') as fp:\n",
        "    line = fp.readline()\n",
        "    while line:\n",
        "        f,_,_,_ = line.split(' ')\n",
        "        res,_ = pat.split(f)\n",
        "        path = os.path.join(path_train_dataset,res)\n",
        "        check_dir(path)\n",
        "        shutil.copy(f'./images/{f}.jpg',os.path.join(path,f'{f}.jpg'))\n",
        "        line = fp.readline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkj3DZjpCdha"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iasXk_FKCdhy"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anZGQpsFZCkP"
      },
      "source": [
        "Now you are ready to redo what we did during lesson 1.\n",
        "\n",
        "Below, you give the path where the data is stored. If you are running this code on your computer, you should modifiy this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJRnJgGOCdh4"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3lE0cvyCdh8"
      },
      "source": [
        "`datasets` is a class of the `torchvision` package (see [torchvision.datasets](https://pytorch.org/vision/main/datasets.html)) and deals with data loading. It integrates a multi-threaded loader that fetches images from the disk, groups them in mini-batches and serves them continously to the GPU right after each _forward_/_backward_ pass through the network.\n",
        "\n",
        "Images needs a bit of preparation before passing them throught the network. They need to have all the same size $224\\times 224 \\times 3$ plus some extra formatting done below by the normalize transform (explained later)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "kwFjoxYEgF8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/data'"
      ],
      "metadata": {
        "id": "NgqYFSQIgHn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "J9ePhNKsgJOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t4vokNrF19p"
      },
      "outputs": [],
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #These values are used to standardize the pixel values of the images to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "vgg_format = transforms.Compose([\n",
        "                transforms.CenterCrop(224), #This crops the central 224x224 pixels from the image. VGG models typically require input images of this size.\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8LMReVECdh-"
      },
      "outputs": [],
      "source": [
        "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), vgg_format)\n",
        "         for x in ['trainval', 'test']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMh7kjEBCdiC"
      },
      "outputs": [],
      "source": [
        "os.path.join(data_dir,'trainval')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8uKAPGAZCkW"
      },
      "source": [
        "We now have 37 different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9ifn_R7CdiH"
      },
      "outputs": [],
      "source": [
        "dsets['trainval'].classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB6sTwFuCdiK"
      },
      "outputs": [],
      "source": [
        "dsets['trainval'].class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WefhjZb2CdiQ"
      },
      "outputs": [],
      "source": [
        "dset_sizes = {x: len(dsets[x]) for x in ['trainval', 'test']}\n",
        "dset_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCLV1YgaCdiT"
      },
      "outputs": [],
      "source": [
        "dset_classes = dsets['trainval'].classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy52-XhACdiX"
      },
      "source": [
        "The `torchvision` packages allows complex pre-processing/transforms of the input data (_e.g._ normalization, cropping, flipping, jittering). A sequence of transforms can be grouped in a pipeline with the help of the `torchvision.transforms.Compose` function, see [torchvision.transforms](https://pytorch.org/vision/main/transforms.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1**: Fill in the following code cell to load the trainval data, with `batch_size` of 64, and `num_workers` of 6."
      ],
      "metadata": {
        "id": "JnQwTTtZ0MYC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWnJQiWgGP_R"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "loader_train = torch.utils.data.DataLoader(dsets['trainval'], batch_size=64, shuffle = True, num_workers = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2**: Fill in the following code cell to load the test data, with batch_size of 5, and num_workers of 6."
      ],
      "metadata": {
        "id": "TblBXcrd0p4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN1BKHfDCdig"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "loader_valid = torch.utils.data.DataLoader(dsets['test'], batch_size=5, shuffle = True, num_workers = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQra6Q7CZCkj"
      },
      "source": [
        "Check your dataloader and everything is doing fine. The following counts the number of batches in `loader_valid`, which should be 734."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Be7lSLCdik"
      },
      "outputs": [],
      "source": [
        "count = 1\n",
        "for data in loader_valid:\n",
        "    print(count, end=',')\n",
        "    if count == 1:\n",
        "        inputs_try,labels_try = data\n",
        "    count += 1\n",
        "\n",
        "    #the reason why it is 734 is because there are a total of 3669 and that means if we are sending the images in groups of 5, it means there will be 734 batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BvxQqfzCdiq"
      },
      "outputs": [],
      "source": [
        "labels_try #labels of the first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code should output `torch.Size([5, 3, 224, 224])`."
      ],
      "metadata": {
        "id": "XhkDIfPI009Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLNsfqc8Cdis"
      },
      "outputs": [],
      "source": [
        "inputs_try.shape #images of the first batch\n",
        "# 5 is the batch-size\n",
        "# 3 is the RGB\n",
        "# 224 is the height and width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsaL21ouKwd9"
      },
      "source": [
        "A small function to display images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "346yl-gbcLLm"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "#   Imshow for Tensor.\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = np.clip(std * inp + mean, 0,1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJbW-QzGCdiv"
      },
      "outputs": [],
      "source": [
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs_try)\n",
        "\n",
        "imshow(out, title=[dset_classes[x] for x in labels_try])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_try #just to check how the tensor looks like"
      ],
      "metadata": {
        "id": "DO2nVwjehR3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HvFNWJICdiz"
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(loader_train))\n",
        "\n",
        "n_images = 8\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
        "\n",
        "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOvkYiROCdi7"
      },
      "source": [
        "## 2. Modifying VGG Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO5LAG4bCdi7"
      },
      "source": [
        "The torchvision module comes with a zoo of popular CNN architectures which are already trained on [ImageNet](http://www.image-net.org/) (1.2M training images). When called the first time, if `pretrained=True` the model is fetched over the internet and downloaded to `~/.torch/models`.\n",
        "For next calls, the model will be directly read from there."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3**: In the following cell, write code to load pretrained VGG16 model."
      ],
      "metadata": {
        "id": "Hw7xEPL30-dA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9PsHjXgCdi9"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models # Import the necessary module\n",
        "\n",
        "#your code here\n",
        "model_vgg = models.vgg16(weights='DEFAULT')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG_total_params = sum(p.numel() for p in model_vgg.parameters())\n",
        "VGG_total_params/1000000"
      ],
      "metadata": {
        "id": "CQ8on4oVZ093"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
      ],
      "metadata": {
        "id": "nqe1YJo5Z90Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "fpath = '/content/data/imagenet_class_index.json'\n",
        "\n",
        "with open(fpath) as f:\n",
        "    class_dict = json.load(f)\n",
        "dic_imagenet = [class_dict[str(i)][1] for i in range(len(class_dict))]"
      ],
      "metadata": {
        "id": "vGwSwZ0AaFLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_imagenet[:4]"
      ],
      "metadata": {
        "id": "16ah5R6baMqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6QQhwruCdjI"
      },
      "outputs": [],
      "source": [
        "inputs_try , labels_try = inputs_try.to(device), labels_try.to(device)\n",
        "\n",
        "model_vgg = model_vgg.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epMB0UF9CdjM"
      },
      "outputs": [],
      "source": [
        "outputs_try = model_vgg(inputs_try)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOlx7YcPCdjO"
      },
      "outputs": [],
      "source": [
        "outputs_try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrweFfK9k0TW"
      },
      "outputs": [],
      "source": [
        "outputs_try.shape #batch of 5, 1000 categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Bktt5PCdjh"
      },
      "source": [
        "### Modifying the last layer and setting the gradient false to all layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8kr3-tjCdji"
      },
      "outputs": [],
      "source": [
        "print(model_vgg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXoMW73MCdjl"
      },
      "source": [
        "We'll learn about what these different blocks do later in the course. For now, it's enough to know that:\n",
        "\n",
        "- Convolution layers are for finding small to medium size patterns in images -- analyzing the images locally\n",
        "- Dense (fully connected) layers are for combining patterns across an image -- analyzing the images globally\n",
        "- Pooling layers downsample -- in order to reduce image size and to improve invariance of learned features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA2f5FRuCdjm"
      },
      "source": [
        "![vgg16](https://dataflowr.github.io/notebooks/Module1/img/vgg16.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK6lfAzfCdjn"
      },
      "source": [
        "Here, our goal is to use the already trained model and just change the number of output classes. To this end we replace the last ```nn.Linear``` layer trained for 1000 classes to ones with 37 classes. In order to freeze the weights of the other layers during training, we set the field ```required_grad=False```. In this manner no gradient will be computed for them during backprop and hence no update in the weights. Only the weights for the 37-class layer will be updated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAr5tj2NKwfF"
      },
      "source": [
        "PyTorch documentation for [LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#logsoftmax)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_softm = nn.Softmax(dim=1)\n",
        "probs = m_softm(outputs_try)\n",
        "vals_try,preds_try = torch.max(probs,dim=1)"
      ],
      "metadata": {
        "id": "QUfeQJLWbsO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "id": "XMpvxsi0btln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(probs,1)"
      ],
      "metadata": {
        "id": "vM4lbhEfbx3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vals_try"
      ],
      "metadata": {
        "id": "KtrRbqIPb0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([dic_imagenet[i] for i in preds_try.data])"
      ],
      "metadata": {
        "id": "6Xopt0CCb3XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = torchvision.utils.make_grid(inputs_try.data.cpu())\n",
        "\n",
        "imshow(out, title=[dset_classes[x] for x in labels_try.data.cpu()])"
      ],
      "metadata": {
        "id": "zHTDeBPKcMBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4**: Write the code to modify the last layer."
      ],
      "metadata": {
        "id": "RvXh9rnrBdGW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQwRKKC-Cdjo"
      },
      "outputs": [],
      "source": [
        "for param in model_vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "# your code here\n",
        "model_vgg.classifier._modules['6'] = nn.Linear(4096, 37)\n",
        "model_vgg.classifier._modules['7'] = torch.nn.LogSoftmax(dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ3OenJpCdjp"
      },
      "outputs": [],
      "source": [
        "print(model_vgg.classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eisaEI7vZClA"
      },
      "source": [
        "**Exercise 5**: Once you modified the architecture of the network, write the code to move the model to GPU for fast training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ITZFX2MCdju"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "model_vgg = model_vgg.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU9vWWT2CdkN"
      },
      "source": [
        "## Training fully connected module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqp2u3IXCdkO"
      },
      "source": [
        "### Creating loss function and optimizer\n",
        "\n",
        "PyTorch documentation for [NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#nllloss) and the [torch.optim module](https://pytorch.org/docs/stable/optim.html#module-torch.optim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6**: Write code to set the optimizer parameter so that only the linear layer just added is trained."
      ],
      "metadata": {
        "id": "IJFDkIXCCIQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP1F4yb8CdkO"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "lr = 0.001\n",
        "# your code here\n",
        "optimizer_vgg = torch.optim.SGD(model_vgg.classifier[6].parameters(),lr = lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tenuLj67CdkS"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nNAUibjCdkS"
      },
      "outputs": [],
      "source": [
        "# Modify the train_model function:\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_model(model,dataloader,size,epochs=1,optimizer=None):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        for inputs,classes in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            classes = classes.to(device)\n",
        "            outputs = model(inputs)\n",
        "            # Apply log_softmax to the outputs before calculating the loss\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "            loss = criterion(log_probs, classes)  # Calculate loss using log probabilities\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _,preds = torch.max(outputs.data,1)\n",
        "            # statistics\n",
        "            running_loss += loss.data.item()\n",
        "            running_corrects += torch.sum(preds == classes.data)\n",
        "        epoch_loss = running_loss / size\n",
        "        epoch_acc = running_corrects.data.item() / size\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                     epoch_loss, epoch_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jts2jK1CdkV",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_model(model_vgg,loader_train,size=dset_sizes['trainval'],epochs=2,optimizer=optimizer_vgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkMYIX21Hc1k"
      },
      "outputs": [],
      "source": [
        "def test_model(model,dataloader,size):\n",
        "    model.eval()\n",
        "    predictions = np.zeros(size)\n",
        "    all_classes = np.zeros(size)\n",
        "    all_proba = np.zeros((size,37))\n",
        "    i = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    #print(size)\n",
        "    for inputs,classes in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs,classes)\n",
        "        _,preds = torch.max(outputs.data,1)\n",
        "            # statistics\n",
        "        running_loss += loss.data.item()\n",
        "        running_corrects += torch.sum(preds == classes.data)\n",
        "        predictions[i:i+len(classes)] = preds.to('cpu').numpy()\n",
        "        all_classes[i:i+len(classes)] = classes.to('cpu').numpy()\n",
        "        all_proba[i:i+len(classes),:] = outputs.data.to('cpu').numpy()\n",
        "        i += len(classes)\n",
        "    epoch_loss = running_loss / size\n",
        "    epoch_acc = running_corrects.data.item() / size\n",
        "    print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                     epoch_loss, epoch_acc))\n",
        "    return predictions, all_proba, all_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3JjYayKCdkW"
      },
      "outputs": [],
      "source": [
        "predictions, all_proba, all_classes = test_model(model_vgg,loader_valid,size=dset_sizes['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn6cIwZNCdkY"
      },
      "outputs": [],
      "source": [
        "# Get a batch of validation data\n",
        "inputs, classes = next(iter(loader_valid))\n",
        "\n",
        "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
        "\n",
        "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model_vgg(inputs[:n_images].to(device))\n",
        "print(torch.exp(outputs))"
      ],
      "metadata": {
        "id": "dTHB71RSdul4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uimC9MvQZClO"
      },
      "source": [
        "**Exercise 7**: Wrte code to compute the predictions made by your network for `inputs[:n_images]` and the associated probabilities.\n",
        "\n",
        "Hint: use `torch.max` and `torch.exp`.\n",
        "\n",
        "Do not forget to put your inputs on the device!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghkbm7ByCdke"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "vals_try, preds_try = vals_try.to(device), preds_try.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LkHVBLrZClQ"
      },
      "outputs": [],
      "source": [
        "preds_try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1qdUtvjCdkg"
      },
      "outputs": [],
      "source": [
        "classes[:n_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJrSi9yiZClS"
      },
      "outputs": [],
      "source": [
        "torch.exp(vals_try)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is your observation? Does the model predict well? Why?"
      ],
      "metadata": {
        "id": "6Zdj4sGgzwyp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0n0hLxTHc1w"
      },
      "source": [
        "## Speeding up the learning by precomputing features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZegyLN3Hc13"
      },
      "outputs": [],
      "source": [
        "def preconvfeat(dataloader):\n",
        "    conv_features = []\n",
        "    labels_list = []\n",
        "    for data in dataloader:\n",
        "        inputs,labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        x = model_vgg.features(inputs)\n",
        "        conv_features.extend(x.data.cpu().numpy())\n",
        "        labels_list.extend(labels.data.cpu().numpy())\n",
        "    conv_features = np.concatenate([[feat] for feat in conv_features])\n",
        "    return (conv_features,labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQiycgsqHc1_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "conv_feat_train,labels_train = preconvfeat(loader_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imSeW6-7Hc2D"
      },
      "outputs": [],
      "source": [
        "conv_feat_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3e5bjsuHc2F"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "conv_feat_valid,labels_valid = preconvfeat(loader_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0fpZByOHc2H"
      },
      "source": [
        "### Creating a new data generator\n",
        "\n",
        "We will not load images anymore, so we need to build our own data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpx-uU90Hc2H"
      },
      "outputs": [],
      "source": [
        "dtype=torch.float\n",
        "datasetfeat_train = [[torch.from_numpy(f).type(dtype),torch.tensor(l).type(torch.long)] for (f,l) in zip(conv_feat_train,labels_train)]\n",
        "datasetfeat_train = [(inputs.reshape(-1), classes) for [inputs,classes] in datasetfeat_train]\n",
        "loaderfeat_train = torch.utils.data.DataLoader(datasetfeat_train, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhrencIvZClZ"
      },
      "source": [
        "Now you can train for more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFNC9PyGHc2P"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_model(model_vgg.classifier,dataloader=loaderfeat_train,size=dset_sizes['trainval'],epochs=80,optimizer=optimizer_vgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAV2Ld2TZClc"
      },
      "outputs": [],
      "source": [
        "datasetfeat_valid = [[torch.from_numpy(f).type(dtype),torch.tensor(l).type(torch.long)] for (f,l) in zip(conv_feat_valid,labels_valid)]\n",
        "datasetfeat_valid = [(inputs.reshape(-1), classes) for [inputs,classes] in datasetfeat_valid]\n",
        "loaderfeat_valid = torch.utils.data.DataLoader(datasetfeat_valid, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CwELM6KZCld"
      },
      "source": [
        "Now you can compute the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yg770yZHc2W"
      },
      "outputs": [],
      "source": [
        "predictions, all_proba, all_classes = test_model(model_vgg.classifier,dataloader=loaderfeat_valid,size=dset_sizes['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFEjbq3DZClf"
      },
      "source": [
        "## Confusion matrix\n",
        "\n",
        "For 37 classes, plotting a confusion matrix is useful to see the performance of the algorithm per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUpzljRGZClg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "def make_fig_cm(cm):\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    tick_marks = np.arange(37);\n",
        "    plt.xticks(tick_marks, dset_classes, rotation=90);\n",
        "    plt.yticks(tick_marks, dset_classes, rotation=0);\n",
        "    plt.tight_layout();\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        coeff = f'{cm[i, j]}'\n",
        "        plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('Actual');\n",
        "    plt.xlabel('Predicted');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY_3JDL9ZClh"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(all_classes,predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE8ARkONZCli"
      },
      "outputs": [],
      "source": [
        "make_fig_cm(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbOU_eNnZClk"
      },
      "source": [
        "**Exercise 8**: What is your observation? Which breeds have the worst predicting performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the classes were showing good performance but the American pitbull terrier, RagDoll and British Shorthair are some of those that were not performing excellent. The worst performing breed is Staffordshire Bull Terrier."
      ],
      "metadata": {
        "id": "pTwKhviZpDcQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shC7RYg8ZClx"
      },
      "source": [
        "## Well done!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}